{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import our previous work and the tf.keras backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_data import *\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the testing/training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]> [0.]\n"
     ]
    }
   ],
   "source": [
    "n_train, n_test = 10000, 2000\n",
    "\n",
    "#generate 20000 training samples\n",
    "x, y = create_data(true_sample_size = n_train + n_test)\n",
    "x_tr, x_v, y_tr, y_v = x[:2*n_train], x[2*n_train:], y[:2*n_train], y[2*n_train:]\n",
    "\n",
    "print(x[0],y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 20000 4000\n",
      "10000 2000\n"
     ]
    }
   ],
   "source": [
    "print(len(y),len(y_tr),len(y_v))\n",
    "print(n_train, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define and compile the model using the adam optimiser and binary_crossentropy loss. (I experimented with dropout here to see how it impacts the results: I got a validation loss of about 1e-2 with dropout and about 1e-5 without dropout. Tweaking the dropout rate may help, but it seems dropout is counterproductive here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poss_chars = ['b', 't', 's', 'x', 'p', 'v', 'e']\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[None,8],dtype=tf.float32,ragged=True),\n",
    "#    keras.layers.Embedding(input_dim=len(poss_chars),output_dim=6),\n",
    "    keras.layers.GRU(32,return_sequences=True,dropout=0.1,\n",
    "                     recurrent_dropout=0.1),\n",
    "    keras.layers.GRU(32,dropout=0.1,recurrent_dropout=0.1),\n",
    "    keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['binary_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.6844 - binary_accuracy: 0.5464 - val_loss: 0.6130 - val_binary_accuracy: 0.8190\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 24s 38ms/step - loss: 0.6440 - binary_accuracy: 0.6237 - val_loss: 0.6010 - val_binary_accuracy: 0.5400\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.5802 - binary_accuracy: 0.6878 - val_loss: 0.2087 - val_binary_accuracy: 0.9630\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.2491 - binary_accuracy: 0.8995 - val_loss: 0.0257 - val_binary_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.1126 - binary_accuracy: 0.9704 - val_loss: 0.0132 - val_binary_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.0836 - binary_accuracy: 0.9804 - val_loss: 0.0115 - val_binary_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.0689 - binary_accuracy: 0.9849 - val_loss: 0.0132 - val_binary_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.0688 - binary_accuracy: 0.9845 - val_loss: 0.0184 - val_binary_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "334/625 [===============>..............] - ETA: 13s - loss: 0.0720 - binary_accuracy: 0.9837"
     ]
    }
   ],
   "source": [
    "model.fit(x_tr,y_tr,epochs=10,validation_data=(x_v,y_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to almost perfectly learn the reber grammar (at least without dropout); we would expect this given the high complexity of the model and the simpleness of the grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I acknowledge this neural net is highly overkill (especially given that I wrote a simple function to do the same task), however I have learned a lot about the tf tools available for preparing data and about some of the hyperparameters available when training RNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
